// Backend Node.js para el Generador LLM.txt con OpenAI - Versi√≥n Corregida
const express = require("express");
const cors = require("cors");
const axios = require("axios");
const rateLimit = require("express-rate-limit");
const helmet = require("helmet");
const cheerio = require("cheerio"); // Agrega esta l√≠nea junto a los otros require
require("dotenv").config();

const app = express();
const PORT = process.env.PORT || 3000;

// Middleware de seguridad
app.use(helmet());
app.use(
  cors({
    origin: true,
    credentials: true, // Permitir cookies y autenticaci√≥n
  })
);
app.use(express.json({ limit: "10mb" }));
app.use(express.static("public"));

// Rate limiting
const limiter = rateLimit({
  windowMs: parseInt(process.env.RATE_LIMIT_WINDOW_MS) || 15 * 60 * 1000, // 15 minutos
  max: parseInt(process.env.RATE_LIMIT_MAX_REQUESTS) || 10, // M√°ximo 10 requests por IP
  message: {
    error:
      "Demasiadas solicitudes desde esta IP, intenta de nuevo en 15 minutos.",
    retryAfter: "15 minutes",
  },
  standardHeaders: true,
  legacyHeaders: false,
});

// Aplica rate limit solo a /api/generate-llm
app.use("/api/generate-llm", limiter);

// Validaci√≥n de URL
function isValidUrl(string) {
  try {
    const url = new URL(string);
    return ["http:", "https:"].includes(url.protocol);
  } catch (error) {
    return false;
  }
}

// Prompt optimizado para OpenAI
function createPrompt(url, scraped = {}) {
  return `Act√∫a como un experto en SEO y an√°lisis web. Tu tarea es analizar la siguiente URL y generar un archivo LLM.txt completo y estructurado.

URL a analizar: ${url}

--- DATOS REALES EXTRA√çDOS ---
T√≠tulo: ${scraped.title || "No encontrado"}
Descripci√≥n: ${scraped.description || "No encontrada"}
H1: ${scraped.h1 || "No encontrado"}
H2: ${scraped.h2 || "No encontrados"}
------------------------------

IMPORTANTE: Usa estos datos reales para mejorar el an√°lisis y las recomendaciones. Si falta informaci√≥n, infiere lo necesario.

# LLM.txt - [T√≠tulo inferido de la URL]

## Informaci√≥n B√°sica del Sitio
- **URL:** ${url}
- **Dominio:** ${new URL(url).hostname}
- **Protocolo:** ${new URL(url).protocol}
- **Fecha de an√°lisis:** ${new Date().toISOString().split("T")[0]}

## An√°lisis de la URL
- **Estructura de URL:** ${
    new URL(url).pathname
      ? "Estructura jer√°rquica clara"
      : "URL base del dominio"
  }
- **Longitud de URL:** ${url.length} caracteres
- **Par√°metros:** ${
    new URL(url).search
      ? "Contiene par√°metros de consulta"
      : "URL limpia sin par√°metros"
  }

## Inferencias Basadas en el Dominio
Bas√°ndome en el dominio "${new URL(url).hostname}":

### Tipo de Sitio Probable
[Inferir si es comercial (.com), educativo (.edu), organizacional (.org), etc.]

### Palabras Clave Potenciales
[Extraer palabras clave del dominio y path de la URL]

### Estructura Recomendada para LLM.txt
1. **Metadatos b√°sicos** - T√≠tulo, descripci√≥n, autor
2. **Contenido principal** - Resumen del prop√≥sito del sitio
3. **Estructura de navegaci√≥n** - Men√∫s principales y p√°ginas clave
4. **Informaci√≥n de contacto** - Datos de la empresa/organizaci√≥n
5. **Contexto SEO** - Keywords principales y mercado objetivo

## Recomendaciones SEO Espec√≠ficas
- **Optimizaci√≥n de URL:** ${
    url.length > 100
      ? "Considerar acortar la URL (>100 caracteres)"
      : "Longitud de URL √≥ptima"
  }
- **Protocolo HTTPS:** ${
    url.startsWith("https")
      ? "‚úÖ Implementado correctamente"
      : "‚ùå Migrar a HTTPS prioritario"
  }
- **Estructura sem√°ntica:** Implementar datos estructurados Schema.org

## Contexto para LLMs
Este archivo LLM.txt est√° dise√±ado para proporcionar contexto sobre el sitio web localizado en ${url}.

### Prop√≥sito Inferido
[Bas√°ndose en la estructura de URL y dominio, inferir el prop√≥sito del sitio]

### Audiencia Objetivo Probable
[Inferir la audiencia bas√°ndose en el tipo de dominio y estructura]

### Contenido Esperado
[Describir qu√© tipo de contenido se esperar√≠a encontrar en este sitio]

## Plantilla de Implementaci√≥n

Para completar este LLM.txt con informaci√≥n real del sitio, incluir:

1. **T√≠tulo real de la p√°gina**
2. **Meta descripci√≥n actual**
3. **Estructura de encabezados (H1-H6)**
4. **Contenido principal resumido**
5. **Enlaces internos importantes**
6. **Informaci√≥n de contacto**
7. **Productos/servicios principales**
8. **Blog/noticias (si aplica)**

## M√©tricas de Monitoreo Sugeridas
- Tiempo de carga de p√°gina
- Core Web Vitals
- Tasa de rebote
- Conversiones (si es e-commerce)
- Tr√°fico org√°nico

---
*Este LLM.txt fue generado autom√°ticamente por IA como estructura base. Se recomienda completar con informaci√≥n espec√≠fica del sitio web real para maximizar su efectividad SEO.*

**Nota:** Para un an√°lisis m√°s preciso, se recomienda inspeccionar manualmente el contenido real del sitio web.`;
}

// Scraping b√°sico de la p√°gina
async function scrapePage(url) {
  try {
    const { data: html } = await axios.get(url, { timeout: 15000 });
    const $ = cheerio.load(html);

    const title = $("title").text().trim();
    const description = $('meta[name="description"]').attr("content") || "";
    const h1 = $("h1").first().text().trim();
    const h2 = $("h2")
      .map((i, el) => $(el).text().trim())
      .get()
      .join(" | ");

    return {
      title,
      description,
      h1,
      h2,
    };
  } catch (error) {
    console.warn("No se pudo scrapear la p√°gina:", error.message);
    return {};
  }
}

// Middleware de logging
app.use("/api/", (req, res, next) => {
  console.log(
    `${new Date().toISOString()} - ${req.method} ${req.path} - IP: ${req.ip}`
  );
  next();
});

// Endpoint principal para generar LLM.txt
app.post("/api/generate-llm", async (req, res) => {
  const startTime = Date.now();

  try {
    const { url } = req.body;

    // Validaciones de entrada
    if (!url) {
      return res.status(400).json({
        error: "URL es requerida",
        code: "MISSING_URL",
      });
    }

    if (typeof url !== "string" || url.trim().length === 0) {
      return res.status(400).json({
        error: "URL debe ser una cadena v√°lida",
        code: "INVALID_URL_FORMAT",
      });
    }

    if (!isValidUrl(url.trim())) {
      return res.status(400).json({
        error: "URL inv√°lida. Debe incluir http:// o https://",
        code: "INVALID_URL",
      });
    }

    // Verificar configuraci√≥n de OpenAI
    if (!process.env.OPENAI_API_KEY) {
      console.error("OPENAI_API_KEY no est√° configurada");
      return res.status(500).json({
        error: "Configuraci√≥n del servidor incompleta",
        code: "MISSING_API_KEY",
      });
    }

    const cleanUrl = url.trim();
    console.log(`Iniciando generaci√≥n de LLM.txt para: ${cleanUrl}`);

    // Scraping antes de generar el prompt
    const scraped = await scrapePage(cleanUrl);
    console.log("Datos scrapeados:", scraped);

    // Configuraci√≥n para la llamada a OpenAI
    const openaiConfig = {
      model: process.env.OPENAI_MODEL || "gpt-4",
      messages: [
        {
          role: "system",
          content:
            "Eres un experto en SEO y an√°lisis web especializado en generar archivos LLM.txt estructurados, √∫tiles y optimizados para motores de b√∫squeda. Generas contenido pr√°ctico, bien formateado y accionable.",
        },
        {
          role: "user",
          content: createPrompt(cleanUrl, scraped), // <-- Aqu√≠ pasamos los datos scrapeados
        },
      ],
      max_tokens: parseInt(process.env.MAX_TOKENS) || 3000,
      temperature: parseFloat(process.env.TEMPERATURE) || 0.3,
      presence_penalty: 0.1,
      frequency_penalty: 0.1,
    };

    // Llamada a OpenAI API con timeout y manejo de errores
    const openaiResponse = await axios.post(
      "https://api.openai.com/v1/chat/completions",
      openaiConfig,
      {
        headers: {
          Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,
          "Content-Type": "application/json",
        },
        timeout: 60000, // 60 segundos
      }
    );

    // Validar respuesta de OpenAI
    if (
      !openaiResponse.data ||
      !openaiResponse.data.choices ||
      !openaiResponse.data.choices[0]
    ) {
      throw new Error("Respuesta inv√°lida de OpenAI API");
    }

    const generatedContent = openaiResponse.data.choices[0].message?.content;

    if (!generatedContent || generatedContent.trim().length === 0) {
      throw new Error("OpenAI no gener√≥ contenido v√°lido");
    }

    // Calcular m√©tricas
    const processingTime = Date.now() - startTime;
    const tokensUsed = openaiResponse.data.usage?.total_tokens || 0;

    // Log de √©xito
    console.log(`‚úÖ LLM.txt generado exitosamente para: ${cleanUrl}`);
    console.log(
      `üìä Tokens utilizados: ${tokensUsed}, Tiempo: ${processingTime}ms`
    );

    // Respuesta exitosa
    res.json({
      success: true,
      content: generatedContent,
      url: cleanUrl,
      generated_at: new Date().toISOString(),
      processing_time_ms: processingTime,
      tokens_used: tokensUsed,
      model_used: openaiConfig.model,
    });
  } catch (error) {
    const processingTime = Date.now() - startTime;
    console.error(
      `‚ùå Error generando LLM.txt (${processingTime}ms):`,
      error.message
    );

    // Manejo espec√≠fico de errores de OpenAI
    if (error.response?.data?.error) {
      const openaiError = error.response.data.error;

      const errorResponses = {
        invalid_api_key: {
          status: 401,
          message: "API Key de OpenAI inv√°lida",
          code: "INVALID_API_KEY",
        },
        insufficient_quota: {
          status: 402,
          message: "Cuota de OpenAI excedida",
          code: "QUOTA_EXCEEDED",
        },
        rate_limit_exceeded: {
          status: 429,
          message: "L√≠mite de velocidad de OpenAI excedido",
          code: "RATE_LIMIT",
        },
        model_not_found: {
          status: 404,
          message: "Modelo de OpenAI no disponible",
          code: "MODEL_NOT_FOUND",
        },
        context_length_exceeded: {
          status: 400,
          message: "Contenido demasiado largo para el modelo",
          code: "CONTENT_TOO_LONG",
        },
      };

      const errorInfo = errorResponses[openaiError.code] || {
        status: 500,
        message: openaiError.message || "Error desconocido de OpenAI",
        code: "OPENAI_ERROR",
      };

      return res.status(errorInfo.status).json({
        error: errorInfo.message,
        code: errorInfo.code,
        processing_time_ms: processingTime,
      });
    }

    // Manejo de errores de red/timeout
    if (error.code === "ECONNABORTED" || error.message.includes("timeout")) {
      return res.status(408).json({
        error: "Timeout: La solicitud tard√≥ demasiado en procesarse",
        code: "TIMEOUT",
        processing_time_ms: processingTime,
      });
    }

    if (error.code === "ENOTFOUND" || error.code === "ECONNREFUSED") {
      return res.status(503).json({
        error: "No se pudo conectar con OpenAI API",
        code: "CONNECTION_ERROR",
        processing_time_ms: processingTime,
      });
    }

    // Error gen√©rico
    res.status(500).json({
      error: "Error interno del servidor",
      code: "INTERNAL_ERROR",
      processing_time_ms: processingTime,
    });
  }
});

// Endpoint de salud del servidor
app.get("/api/health", (req, res) => {
  const uptime = process.uptime();
  const memoryUsage = process.memoryUsage();

  res.json({
    status: "OK",
    timestamp: new Date().toISOString(),
    uptime_seconds: Math.floor(uptime),
    memory: {
      used: Math.round(memoryUsage.heapUsed / 1024 / 1024) + " MB",
      total: Math.round(memoryUsage.heapTotal / 1024 / 1024) + " MB",
    },
    version: "1.0.0",
    node_version: process.version,
  });
});

// Endpoint para verificar configuraci√≥n
app.get("/api/config", (req, res) => {
  res.json({
    openai_configured: !!process.env.OPENAI_API_KEY,
    model: process.env.OPENAI_MODEL || "gpt-4",
    max_tokens: parseInt(process.env.MAX_TOKENS) || 3000,
    temperature: parseFloat(process.env.TEMPERATURE) || 0.3,
    rate_limit: `${
      parseInt(process.env.RATE_LIMIT_MAX_REQUESTS) || 10
    } requests per ${Math.floor(
      (parseInt(process.env.RATE_LIMIT_WINDOW_MS) || 900000) / 60000
    )} minutes`,
    environment: process.env.NODE_ENV || "development",
  });
});

// Ruta ra√≠z
app.get("/", (req, res) => {
  res.json({
    message: "ü§ñ Generador LLM.txt con IA - API funcionando",
    version: "1.0.0",
    endpoints: {
      generate: "POST /api/generate-llm",
      health: "GET /api/health",
      config: "GET /api/config",
    },
  });
});

// Manejo de rutas no encontradas
app.use((req, res, next) => {
  res.status(404).json({
    error: "Endpoint no encontrado",
    path: req.path,
    method: req.method,
    available_endpoints: [
      "POST /api/generate-llm",
      "GET /api/health",
      "GET /api/config",
      "GET /",
    ],
  });
});

// Manejo global de errores
app.use((error, req, res, next) => {
  console.error("Error no manejado:", error);
  res.status(500).json({
    error: "Error interno del servidor",
    code: "UNHANDLED_ERROR",
    timestamp: new Date().toISOString(),
  });
});

// Manejo de se√±ales del sistema
process.on("SIGTERM", () => {
  console.log("SIGTERM recibido, cerrando servidor...");
  process.exit(0);
});

process.on("SIGINT", () => {
  console.log("SIGINT recibido, cerrando servidor...");
  process.exit(0);
});

// Iniciar servidor
app.listen(PORT, () => {
  console.log("");
  console.log("üöÄ ========================================");
  console.log(`   Servidor iniciado en puerto ${PORT}`);
  console.log("üöÄ ========================================");
  console.log(`üìä Endpoints disponibles:`);
  console.log(`   ‚Ä¢ http://localhost:${PORT}/`);
  console.log(`   ‚Ä¢ http://localhost:${PORT}/api/generate-llm`);
  console.log(`   ‚Ä¢ http://localhost:${PORT}/api/health`);
  console.log(`   ‚Ä¢ http://localhost:${PORT}/api/config`);
  console.log("");
  console.log(`üîß Configuraci√≥n:`);
  console.log(
    `   ‚Ä¢ OpenAI: ${
      !!process.env.OPENAI_API_KEY ? "‚úÖ Configurado" : "‚ùå No configurado"
    }`
  );
  console.log(`   ‚Ä¢ Modelo: ${process.env.OPENAI_MODEL || "gpt-4"}`);
  console.log(
    `   ‚Ä¢ Rate Limit: ${
      parseInt(process.env.RATE_LIMIT_MAX_REQUESTS) || 10
    } req/15min`
  );
  console.log(`   ‚Ä¢ Entorno: ${process.env.NODE_ENV || "development"}`);
  console.log("");

  if (!process.env.OPENAI_API_KEY) {
    console.log("‚ö†Ô∏è  ADVERTENCIA: OPENAI_API_KEY no est√° configurada");
    console.log("   Crea un archivo .env con tu API key de OpenAI");
    console.log("");
  }
});

module.exports = app;
